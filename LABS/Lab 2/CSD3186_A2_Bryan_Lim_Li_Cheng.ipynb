{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3ca3339",
   "metadata": {},
   "source": [
    "__Assignment 2__  \n",
    "__Name: Bryan Lim Li Cheng__  \n",
    "__Student ID: 2301214__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7207c747",
   "metadata": {},
   "source": [
    "# CSD3185/CSD3186: Assignment 2\n",
    "\n",
    "## Topics Covered:  \n",
    "- Data Preprocessing\n",
    "- Logistic Regression Classifier\n",
    "- K-Nearest Neighbors (KNN) Classifier\n",
    "- Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e410d78d",
   "metadata": {},
   "source": [
    "## Python and Dependency Versions  \n",
    "To ensure reproducibility and compatibility, please check that you have the following Python and dependency versions installed:\n",
    "\n",
    "- Python >= 3.12\n",
    "- pandas == 2.3.3\n",
    "- numpy == 2.4.1\n",
    "- scikit-learn == 1.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e73562",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "In this assignment, we'll use a bank marketing dataset, which focuses on direct marketing campaigns conducted via phone calls by a Portuguese banking institution. The objective is to predict whether a client will subscribe to a term deposit (coded as 1 for yes or 0 for no).\n",
    "\n",
    "**IMPORTANT**: Download and use dataset provided in the Moodle for this assignment. Do NOT use any other version of the dataset available online as they may differ in structure and content.\n",
    "\n",
    "#### **Attribute Overview**   \n",
    "|**Input Feature**     |**Description**                         |\n",
    "|:-----------------------|:-------------------------------------------------------------------------------------------------------------------|\n",
    "| `age`                | Age                                                                                                                     |\n",
    "| `job`                | Type of job                                                                                                             |\n",
    "| `marital`            | Marital status                                                                                                          |\n",
    "| `education`          | Education level                                                                                                         |\n",
    "| `default`            | Has credit in default?                                                                                                  |\n",
    "| `housing`            | Has housing loan?                                                                                                       |\n",
    "| `loan`               | Has personal loan?                                                                                                      |\n",
    "| `contact`            | Contact communication type                                                                                              |\n",
    "| `month`              | Last contact month of year                                                                                              |\n",
    "| `day_of_week`        | Last contact day of the week                                                                                            |\n",
    "| `duration`           | Last contact duration in seconds.\n",
    "| `campaign`           | Number of contacts performed during this campaign for this client                                                       |\n",
    "| `pdays`              | Number of days since last contact from a previous campaign (999 = not previously contacted)                             |\n",
    "| `previous`           | Number of contacts performed before this campaign for this client                                                       |\n",
    "| `poutcome`           | Outcome of the previous marketing campaign                                                                              |\n",
    "| `emp.var.rate`       | Employment variation rate                                                                                               | \n",
    "| `cons.price.idx`     | Consumer price index                                                                                                    |\n",
    "| `cons.conf.idx`      | Consumer confidence index                                                                                               |\n",
    "| `euribor3m`          | Euribor 3-month rate                                                                                                    |\n",
    "| `nr.employed`        | Number of employees                                                                                                     |\n",
    "| **Target variable**  | **Description**                                                                                                         |\n",
    "| `y`                  | Has the client subscribed to a term deposit?                                                                            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed5f2e9",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "\n",
    "Your submission for this assignment should be only __ONE__ file - this particular completed notebook file. \n",
    "\n",
    "Also, *RENAME* your file like this: __\\<coursecode\\>\\_<assignment#>\\_<your_full_name>.ipynb__  \n",
    "Eg. CS3185_A2_John_Doe.ipynb  \n",
    "\n",
    "To complete this assignment, you should follow instructions in below section Tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42709b3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## IMPORTANT! READ THIS BEFORE STARTING...\n",
    "- DO NOT delete existing cells, but you can add more cells anywhere in the notebook as necessary.\n",
    "- DO NOT modify or comment out the content of the existing cells unless otherwise stated (e.g., for code implementation). However, DO NOT change the variable names that are already defined in the existing cells.\n",
    "- Follow the file naming convention for the notebook file as spelled out above strictly.\n",
    "\n",
    "Please adhere strictly to the instructions as stated above as failure to do so might result in deduction of marks by the autograder.\n",
    "\n",
    "Your assignment begins after the line below!! Complete all the tasks as specified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e67406",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff37eb41",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df624f3",
   "metadata": {},
   "source": [
    "Load some basic libraries upfront. You may add any other libraries you deem necessary below or later on where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91d45531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bade780",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "__Task 1.1__\n",
    "\n",
    "Load the banking dataset from the provided CSV file into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bfd4cb9",
   "metadata": {
    "otter": {
     "tests": [
      "Q1.1"
     ]
    }
   },
   "outputs": [],
   "source": [
    "# read the dataset as a pandas DataFrame into `data`\n",
    "# place banking.csv in the same folder as this notebook; load it by filename only (no paths).\n",
    "data = pd.read_csv(\"banking.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0425cad9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef281b93",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "__Task 1.2__\n",
    "\n",
    "Perform a few basic data exploration steps specified to understand the structure and content of the dataset (use pandas DataFrame methods):\n",
    "\n",
    "- Understand the dataset structure (number of rows and columns).\n",
    "- Calculate and display the number of missing values in each column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bd34441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41188, 21)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_shape = data.shape\n",
    "data_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c63fab7",
   "metadata": {
    "otter": {
     "tests": [
      "Q1.2"
     ]
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               0\n",
       "job               0\n",
       "marital           0\n",
       "education         0\n",
       "default           0\n",
       "housing           0\n",
       "loan              0\n",
       "contact           0\n",
       "month             0\n",
       "day_of_week       0\n",
       "duration          0\n",
       "campaign          0\n",
       "pdays             0\n",
       "previous          0\n",
       "poutcome          0\n",
       "emp_var_rate      0\n",
       "cons_price_idx    0\n",
       "cons_conf_idx     0\n",
       "euribor3m         0\n",
       "nr_employed       0\n",
       "y                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values = data.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48978bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "190160e6",
   "metadata": {},
   "source": [
    "__Task 1.3__\n",
    "\n",
    "The dataset contains categorical features. Display all unique values for each categorical feature in the dataset to understand the possible categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aaea9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "job: ['blue-collar' 'technician' 'management' 'services' 'retired' 'admin.'\n",
      " 'housemaid' 'unemployed' 'entrepreneur' 'self-employed' 'unknown'\n",
      " 'student']\n",
      "\n",
      "marital: ['married' 'single' 'divorced' 'unknown']\n",
      "\n",
      "education: ['basic.4y' 'unknown' 'university.degree' 'high.school' 'basic.9y'\n",
      " 'professional.course' 'basic.6y' 'illiterate']\n",
      "\n",
      "default: ['unknown' 'no' 'yes']\n",
      "\n",
      "housing: ['yes' 'no' 'unknown']\n",
      "\n",
      "loan: ['no' 'yes' 'unknown']\n",
      "\n",
      "contact: ['cellular' 'telephone']\n",
      "\n",
      "month: ['aug' 'nov' 'jun' 'apr' 'jul' 'may' 'oct' 'mar' 'sep' 'dec']\n",
      "\n",
      "day_of_week: ['thu' 'fri' 'tue' 'mon' 'wed']\n",
      "\n",
      "poutcome: ['nonexistent' 'success' 'failure']\n"
     ]
    }
   ],
   "source": [
    "# Use a dictionary of unique values\n",
    "\n",
    "categorical_cols = data.select_dtypes(include=[\"object\"]).columns\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}: {data[col].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423384c2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90da8b54",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "__Task 1.4__\n",
    "\n",
    "The education column contains various levels of education. We can simplify this by grouping similar education levels together.\n",
    "\n",
    "Group \"basic.4y\", \"basic.6y\", and \"basic.9y\" into a single category called \"basic\" in the `data` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2198e478",
   "metadata": {
    "otter": {
     "tests": [
      "Q1.3"
     ]
    }
   },
   "outputs": [],
   "source": [
    "data['education'] = data['education'].replace(['basic.4y','basic.6y','basic.9y'],'basic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096b0bfb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59f39b4b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "__Task 1.5__\n",
    "\n",
    "Analyze the target variable y to understand its distribution and derive some basic insights. This analysis helps assess class imbalance and potential relationships between the target and other variables.\n",
    "\n",
    "- Display the value counts of the target variable y using the `value_counts()` method\n",
    "- Calculate the number and percentage of rows where y is 0 (no subscription) and 1 (subscription).\n",
    "- Display the mean of numeric columns grouped by the target variable y using the `groupby()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5a69d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts for the target variable 'y':\n",
      "y\n",
      "0    36548\n",
      "1     4640\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display value counts for the target variable\n",
    "y_value_counts = data[\"y\"].value_counts()\n",
    "print(\"Value counts for the target variable 'y':\")\n",
    "print(y_value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b47de595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of no subscription: 88.73%\n",
      "Percentage of subscription: 11.27%\n"
     ]
    }
   ],
   "source": [
    "# Calculate subscription statistics\n",
    "count_no_sub = (data[\"y\"] == 0).sum()\n",
    "count_sub = (data[\"y\"] == 1).sum()\n",
    "\n",
    "pct_of_no_sub = count_no_sub / len(data)\n",
    "pct_of_sub = count_sub / len(data)\n",
    "\n",
    "print(f\"Percentage of no subscription: {pct_of_no_sub * 100:.2f}%\")\n",
    "print(f\"Percentage of subscription: {pct_of_sub * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b3f4809",
   "metadata": {
    "otter": {
     "tests": [
      "Q1.4"
     ]
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of numeric columns grouped by 'y':\n",
      "         age    duration  campaign       pdays  previous  emp_var_rate  \\\n",
      "y                                                                        \n",
      "0  39.911185  220.844807  2.633085  984.113878  0.132374      0.248875   \n",
      "1  40.913147  553.191164  2.051724  792.035560  0.492672     -1.233448   \n",
      "\n",
      "   cons_price_idx  cons_conf_idx  euribor3m  nr_employed  \n",
      "y                                                         \n",
      "0       93.603757     -40.593097   3.811491  5176.166600  \n",
      "1       93.354386     -39.789784   2.123135  5095.115991  \n"
     ]
    }
   ],
   "source": [
    "# Display the mean of numeric columns grouped by the target variable\n",
    "grouped_means = data.groupby(\"y\").mean(numeric_only = True)\n",
    "\n",
    "print(\"\\nMean of numeric columns grouped by 'y':\")\n",
    "print(grouped_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca27c259",
   "metadata": {},
   "source": [
    "__Task 1.6__\n",
    "\n",
    "Based on the outputs observe in previous steps, write a brief summary of your findings regarding the target variable distribution and any notable patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29618c7",
   "metadata": {},
   "source": [
    "# Write your summary in this markdown cell here.  \n",
    "The dataset shows extreme class imbalance with only 11.27% subscription rate reflects the challenge of converting prospects through phone calls. Subscribers show significantly longer call durations of 553 seconds, compared to 221 seconds of non-subscribers. This suggests that conversations quality matters more than frequent contacts. Economic indicators such as including employment variation rate, Euribor interest rates, and consumer confidenceâ€”show clear patterns, with favorable conditions correlating with higher conversion rates. Customer demographics span various occupations, education levels, and marital statuses, while previous campaign outcomes and contact history also influence subscription likelihood, indicating relationship-building improves conversion success."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71268ce3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27ddbbe3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "__Task 1.7__\n",
    "\n",
    "Categorical features often need to be encoded into numerical format for machine learning algorithms. Use one-hot encoding to convert all categorical features in the dataset into numerical format. \n",
    "\n",
    "Use the `pd.get_dummies()` function from pandas to achieve this. This approach is widely used to convert categorical variables into a format suitable for machine learning models. To learn more about pd.get_dummies() and its usage, refer to the [pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb80d209",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Create a function named `encode_categorical_features` that takes a pandas DataFrame as input and returns a new DataFrame with all categorical features one-hot encoded.\n",
    "\n",
    "For the new column names generated by one-hot encoding, use the format `<column name>_<category value>`. For example, if the original column is `job` and one of its categories is `admin.`, the new column should be named `job_admin.`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4149ad0",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Complete the function to one-hot encode categorical features\n",
    "def encode_categorical_features(df):\n",
    "    \"\"\"One-hot encode all categorical features in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame with categorical features.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: New DataFrame with one-hot encoded categorical features.\n",
    "    \"\"\"\n",
    "    ...\n",
    "    return encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadc0420",
   "metadata": {
    "otter": {
     "tests": [
      "Q1.5"
     ]
    }
   },
   "outputs": [],
   "source": [
    "# Apply the encoding function to the dataset\n",
    "data_encoded = encode_categorical_features(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe8a4ae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15ceb97a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "__Task 1.8__\n",
    "\n",
    "The dataset have class imbalance as seen in the target variable distribution. Class imbalance can affect the performance of machine learning models. To address this, there are several techniques that can be employed, but for this assignment, we will focus on one specific technique: __over-sampling__.\n",
    "\n",
    "Over-sampling, which is the process of randomly duplicating observations from the minority class to achieve a balanced dataset. The most common approach to over-sampling is to resample with replacement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b917fb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Perform over-sampling on the training data only. This is important to avoid data leakage and ensure that the model is evaluated on unseen data.\n",
    "\n",
    "To help with this task, create a function named `split_data` which helps to split the dataset into training and testing sets. Use `train_test_split` inside and ensure to maintain the same percentage of class distribution in both sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c72e6e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Subsequently, implement a function named `oversample_minority_class` that takes the training set and the name of the target column as input. The function should return a new DataFrame with balanced classes in the target variable. Use the following steps in your implementation:\n",
    "\n",
    "- Separate the majority and minority classes in the training data.\n",
    "- Upsample the minority class by randomly duplicating its samples.\n",
    "- Combine the upsampled minority class with the majority class to create a balanced dataset.\n",
    "\n",
    "Note: Use the `resample` function from `sklearn.utils` to perform the upsampling. NO other libraries for handling imbalanced datasets should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825a190b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7180ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the function\n",
    "def split_data(df, target_column, test_size, random_state):\n",
    "    \"\"\"Split the DataFrame into training and testing sets.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        target_column (str): Name of the target column.\n",
    "        test_size (float): Proportion of the dataset to include in the test split.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame, pd.DataFrame: train_data, test_data (including target)\n",
    "    \"\"\"\n",
    "    ...\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90418f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the encoded data into training and testing sets 80-20 split\n",
    "train_data, test_data = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f60f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shapes of the resulting datasets\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fef024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the function\n",
    "def oversample_minority_class(df, target_col, random_state):\n",
    "    \"\"\"Over-sample the minority class in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame with imbalanced classes.\n",
    "        target_col (str): Name of the target column.\n",
    "        random_state (int): Random state for reproducibility.\n",
    "    Returns:\n",
    "        pd.DataFrame: New DataFrame with balanced classes.\n",
    "    \"\"\"\n",
    "    ...\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c634f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over-sample the minority class in the training data\n",
    "train_data_balanced = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985626d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check train_data_balanced class distribution\n",
    "balanced_class_distribution = ...\n",
    "balanced_class_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051046c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c8d8556",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "__Task 1.9__\n",
    "\n",
    "Scale the features in the training and testing sets using `MinMaxScaler` from sklearn. \n",
    "\n",
    "Create a function named `scale_features` that takes the training and testing DataFrames as input and returns the scaled versions of both DataFrames. Ensure that the scaler is fitted only on the training data to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbd4e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the function\n",
    "def scale_features(train_df, test_df):\n",
    "    \"\"\"Scale features in training and testing DataFrames using MinMaxScaler.\n",
    "    \n",
    "    Args:\n",
    "        train_df (pd.DataFrame): Training DataFrame.\n",
    "        test_df (pd.DataFrame): Testing DataFrame.\n",
    "    Returns:\n",
    "        pd.DataFrame, pd.DataFrame: Scaled training and testing DataFrames.\n",
    "    \"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b652417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable in training and testing sets\n",
    "X_train = ...\n",
    "y_train = ...\n",
    "\n",
    "X_test = ...\n",
    "y_test = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42597818",
   "metadata": {
    "otter": {
     "tests": [
      "Q1.7"
     ]
    }
   },
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "X_train_scaled, X_test_scaled = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943db418",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbfcb65",
   "metadata": {},
   "source": [
    "## 2. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970cd249",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "__Task 2.1__\n",
    "\n",
    "Build a baseline logistic regression model and a K-Nearest Neighbors (KNN) model using the scaled training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc47d0e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740c330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate logistic regression model with all default params\n",
    "base_logreg = ...\n",
    "\n",
    "# Fit the model on the training data\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787a0b0d",
   "metadata": {
    "otter": {
     "tests": [
      "Q2.1"
     ]
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate KNN model with all default params\n",
    "base_knn = ...\n",
    "\n",
    "# Fit the model on the training data\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcec194e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffa67ab8",
   "metadata": {},
   "source": [
    "__Task 2.2__\n",
    "\n",
    "Evaluate both models on the scaled testing data using accuracy, precision, recall, and F1-score as metrics. Use the appropriate functions from `sklearn.metrics` to compute these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ac5de9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b412aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd184965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8753f97",
   "metadata": {},
   "source": [
    "__Task 2.3__\n",
    "\n",
    "Discuss your observations based on the evaluation results using Markdown cells below. (less than 100 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3297934f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69af52bf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "766233b6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fb4206f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e767a721",
   "metadata": {},
   "source": [
    "## 3. Tuning Your Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255b8bb1",
   "metadata": {},
   "source": [
    "Use `GridSearchCV` from sklearn to perform hyperparameter tuning for both the logistic regression and KNN models. Define a grid of hyperparameters to search over for each model.\n",
    "\n",
    "The goal is to improve the performance of both models through hyperparameter tuning. Hence, try to obtain a better performing model based on accuracy.\n",
    "\n",
    "However, take note that hyperparameter tuning can be computationally expensive. To manage this, limit the number of hyperparameter combinations by selecting only a few key hyperparameters and a small range of values for each. You can experiment with any amount of different hyperparameter values when you tune your models in your machine locally. But for submission, keep the grid search reasonable (include the best hyperparameters during your experiment) to ensure that it can complete in a timely manner. \n",
    "\n",
    "After completing the grid search, save the best models for both logistic regression and KNN based on cross-validation performance to the variables `best_logreg_model` and `best_knn_model` respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18120aa",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "__Task 3.1__\n",
    "\n",
    "Perform hyperparameter tuning for the logistic regression model using `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed2adcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a657aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486a48f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your best model from the grid search here\n",
    "best_logreg_model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3813ed",
   "metadata": {
    "otter": {
     "tests": [
      "Q3.1"
     ]
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the best model on the test set (DO NOT modify this cell)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_best_logreg_test = best_logreg_model.predict(X_test_scaled)\n",
    "best_logreg_test_accuracy = accuracy_score(y_test, y_pred_best_logreg_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8618804b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6eddc41a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "__Task 3.2__\n",
    "\n",
    "Perform hyperparameter tuning for the KNN model using `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780e8121",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2326dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your best model from the grid search here\n",
    "best_knn_model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3cf378",
   "metadata": {
    "otter": {
     "tests": [
      "Q3.2"
     ]
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the best model on the test set (DO NOT modify this cell)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_best_knn_test = best_knn_model.predict(X_test_scaled)\n",
    "best_knn_test_accuracy = accuracy_score(y_test, y_pred_best_knn_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62584ca9",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; gap: 8px; margin: 12px 0; color: #868181;\">\n",
    "  <hr style=\"flex: 1; border: none; border-top: 1px solid #ccc;\">\n",
    "  <span style=\"font-size: 1.2em;\">END ASSIGNMENT</span>\n",
    "  <hr style=\"flex: 1; border: none; border-top: 1px solid #ccc;\">\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "Q1.1": {
     "name": "Q1.1",
     "points": null,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "Q1.2": {
     "name": "Q1.2",
     "points": null,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "Q1.3": {
     "name": "Q1.3",
     "points": null,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "Q1.4": {
     "name": "Q1.4",
     "points": null,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "Q1.5": {
     "name": "Q1.5",
     "points": null,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "Q1.6": {
     "name": "Q1.6",
     "points": null,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "Q1.7": {
     "name": "Q1.7",
     "points": null,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "Q2.1": {
     "name": "Q2.1",
     "points": null,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "Q3.1": {
     "name": "Q3.1",
     "points": null,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "Q3.2": {
     "name": "Q3.2",
     "points": null,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
